{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2d0709-f977-4411-9b14-4870782d827e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# =============================\n",
    "# 1. Load Dataset\n",
    "# =============================\n",
    "df = pd.read_csv(\"kirulapone_hourly_data.csv\", parse_dates=['date'])\n",
    "series = df['pm2_5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0acd8fb-01f4-4b9b-83ba-2a040f0ebc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Dataset Statistics\n",
    "# =============================\n",
    "mean_pm25 = series.mean()\n",
    "median_pm25 = series.median()\n",
    "std_pm25 = series.std()\n",
    "max_pm25 = series.max()\n",
    "min_pm25 = series.min()\n",
    "num_observations = series.count()\n",
    "date_range = (df['date'].min(), df['date'].max())\n",
    "\n",
    "# Print results\n",
    "print(\"============================================================\")\n",
    "print(\"KIRULAPONE PM2.5 DATASET STATISTICS\")\n",
    "print(\"============================================================\")\n",
    "print(f\"MEAN PM2.5:     {mean_pm25:.2f} Âµg/mÂ³\")\n",
    "print(f\"MEDIAN:         {median_pm25:.2f} Âµg/mÂ³\")\n",
    "print(f\"STD DEV:        {std_pm25:.2f} Âµg/mÂ³\")\n",
    "print(f\"MAX:            {max_pm25:.2f} Âµg/mÂ³\")\n",
    "print(f\"MIN:            {min_pm25:.2f} Âµg/mÂ³\")\n",
    "print(f\"OBSERVATIONS:   {num_observations}\")\n",
    "print(f\"DATE RANGE:     {date_range[0].date()} to {date_range[1].date()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dad7a65-2949-4408-b579-a0ee2a03f168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier Detection\n",
    "# Z-score\n",
    "z_scores = (series - series.mean()) / series.std()\n",
    "z_outliers = series[abs(z_scores) > 3]\n",
    "\n",
    "# IQR\n",
    "Q1, Q3 = series.quantile(0.25), series.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "iqr_outliers = series[(series < Q1 - 1.5*IQR) | (series > Q3 + 1.5*IQR)]\n",
    "\n",
    "# Rolling Z-score\n",
    "rolling_mean = series.rolling(7, center=True).mean()\n",
    "rolling_std = series.rolling(7, center=True).std()\n",
    "rolling_z = (series - rolling_mean) / rolling_std\n",
    "rolling_outliers = series[abs(rolling_z) > 3]\n",
    "\n",
    "# Summary\n",
    "print(\"\\n--- OUTLIER COUNTS ---\")\n",
    "print(f\"â€¢ Z-score method (>3Ïƒ): {len(z_outliers)} outliers\")\n",
    "print(f\"â€¢ IQR method (1.5*IQR): {len(iqr_outliers)} outliers\")\n",
    "print(f\"â€¢ Rolling Z-score:      {len(rolling_outliers)} outliers\")\n",
    "\n",
    "print(\"\\n--- Percentages ---\")\n",
    "print(f\"â€¢ Z-score: {len(z_outliers)/len(series)*100:.2f}%\")\n",
    "print(f\"â€¢ IQR:     {len(iqr_outliers)/len(series)*100:.2f}%\")\n",
    "print(f\"â€¢ Rolling: {len(rolling_outliers)/len(series)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd1a9c2-662b-41a0-820e-fec4255a112d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# 2. Train/Test Split\n",
    "# =============================\n",
    "test_size = 24 * 30  # last 30 days\n",
    "train = df.iloc[:-test_size].copy()\n",
    "test = df.iloc[-test_size:].copy()\n",
    "\n",
    "# =============================\n",
    "# 3. Light Clipping (fit on train only)\n",
    "# =============================\n",
    "clip_threshold = train['pm2_5'].quantile(0.95)\n",
    "train['pm2_5_clipped'] = np.minimum(train['pm2_5'], clip_threshold)\n",
    "test['pm2_5_clipped'] = np.minimum(test['pm2_5'], clip_threshold)\n",
    "\n",
    "# =============================\n",
    "# 4. Log Transform\n",
    "# =============================\n",
    "train['pm2_5_log'] = np.log(train['pm2_5_clipped'] + 1)\n",
    "test['pm2_5_log'] = np.log(test['pm2_5_clipped'] + 1)\n",
    "\n",
    "# =============================\n",
    "# 5. Scaling\n",
    "# =============================\n",
    "scaler = MinMaxScaler()\n",
    "train['pm2_5_scaled'] = scaler.fit_transform(train[['pm2_5_log']])\n",
    "test['pm2_5_scaled'] = scaler.transform(test[['pm2_5_log']])\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"DATA PREPROCESSING COMPLETE\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")\n",
    "print(f\"Clipping threshold: {clip_threshold:.2f}\")\n",
    "print(f\"Train scaled range: {train['pm2_5_scaled'].min():.4f} to {train['pm2_5_scaled'].max():.4f}\")\n",
    "print(f\"Test scaled range: {test['pm2_5_scaled'].min():.4f} to {test['pm2_5_scaled'].max():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66989b17-d909-4a90-82ff-071f8ea3cef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Create Sequences for LSTM\n",
    "# =============================\n",
    "def create_sequences(data, seq_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data[i:i + seq_length])\n",
    "        y.append(data[i + seq_length])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "SEQ_LENGTH = 24\n",
    "\n",
    "train_data = train['pm2_5_scaled'].values\n",
    "test_data = test['pm2_5_scaled'].values\n",
    "\n",
    "X_train, y_train = create_sequences(train_data, SEQ_LENGTH)\n",
    "X_test, y_test = create_sequences(test_data, SEQ_LENGTH)\n",
    "\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "\n",
    "# =============================\n",
    "# Build LSTM Model\n",
    "# =============================\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, activation='tanh', return_sequences=True, \n",
    "               recurrent_dropout=0.25, kernel_regularizer=l2(0.001),\n",
    "               input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dropout(0.35))\n",
    "model.add(LSTM(16, activation='tanh', recurrent_dropout=0.25, \n",
    "               kernel_regularizer=l2(0.001)))\n",
    "model.add(Dropout(0.35))\n",
    "model.add(Dense(1))\n",
    "\n",
    "optimizer = Adam(learning_rate=0.0006, clipnorm=1.0)\n",
    "model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "model.summary()\n",
    "\n",
    "# =============================\n",
    "# Callbacks & Training\n",
    "# =============================\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, \n",
    "                          restore_best_weights=True, min_delta=0.0001, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.6, \n",
    "                             patience=4, min_lr=0.00001, verbose=1)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=128,\n",
    "                   validation_data=(X_test, y_test),\n",
    "                   callbacks=[early_stop, reduce_lr], verbose=1)\n",
    "\n",
    "# =============================\n",
    "# Predictions & Evaluation\n",
    "# =============================\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "y_pred_log = scaler.inverse_transform(y_pred)\n",
    "y_test_log = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "y_pred_original = np.exp(y_pred_log) - 1\n",
    "y_test_original = np.exp(y_test_log) - 1\n",
    "\n",
    "mse = mean_squared_error(y_test_original, y_pred_original)\n",
    "mae = mean_absolute_error(y_test_original, y_pred_original)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test_original, y_pred_original)\n",
    "mape = np.mean(np.abs((y_test_original - y_pred_original) / y_test_original)) * 100\n",
    "\n",
    "print(f\"\\nMSE: {mse:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}, RÂ²: {r2:.4f}, MAPE: {mape:.4f}%\")\n",
    "print(f\"Sample predictions: {y_pred_original[:5].flatten()}\")\n",
    "print(f\"Sample actuals: {y_test_original[:5].flatten()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cef2bae-a1c3-4439-add8-df02dd3ace00",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epochs', fontsize=12)\n",
    "plt.ylabel('Loss (MSE)', fontsize=12)\n",
    "plt.title('Training vs Validation Loss', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1c8e5a-d65a-4b1f-bc14-0e23797d0cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# STORE LSTM RESULTS\n",
    "# =============================\n",
    "# After LSTM predictions\n",
    "y_pred_lstm = y_pred_original.copy()\n",
    "mse_lstm = mse\n",
    "mae_lstm = mae\n",
    "rmse_lstm = rmse\n",
    "r2_lstm = r2\n",
    "mape_lstm = mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01148edb-3edf-45a5-91f0-46c721caca8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "# =============================\n",
    "# N-BEATS Block Definition\n",
    "# =============================\n",
    "class NBeatsBlock(layers.Layer):\n",
    "    def __init__(self, units, theta_dim, horizon, lookback, l2_reg=0.005, dropout_rate=0.4, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.theta_dim = theta_dim\n",
    "        self.horizon = horizon\n",
    "        self.lookback = lookback\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        self.fc1 = layers.Dense(units, activation='relu', kernel_regularizer=l2(l2_reg))\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.dropout1 = layers.Dropout(dropout_rate)\n",
    "        \n",
    "        self.fc2 = layers.Dense(units // 2, activation='relu', kernel_regularizer=l2(l2_reg))\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        self.dropout2 = layers.Dropout(dropout_rate)\n",
    "        \n",
    "        self.theta_b = layers.Dense(theta_dim, activation='linear', use_bias=False, \n",
    "                                     kernel_regularizer=l2(l2_reg))\n",
    "        self.theta_f = layers.Dense(theta_dim, activation='linear', use_bias=False,\n",
    "                                     kernel_regularizer=l2(l2_reg))\n",
    "        \n",
    "        self.backcast_linear = layers.Dense(lookback, use_bias=False, \n",
    "                                           kernel_regularizer=l2(l2_reg))\n",
    "        self.forecast_linear = layers.Dense(horizon, use_bias=False,\n",
    "                                           kernel_regularizer=l2(l2_reg))\n",
    "    \n",
    "    def call(self, x, training=None):\n",
    "        h = self.fc1(x)\n",
    "        h = self.bn1(h, training=training)\n",
    "        h = self.dropout1(h, training=training)\n",
    "        \n",
    "        h = self.fc2(h)\n",
    "        h = self.bn2(h, training=training)\n",
    "        h = self.dropout2(h, training=training)\n",
    "        \n",
    "        theta_b = self.theta_b(h)\n",
    "        theta_f = self.theta_f(h)\n",
    "        \n",
    "        backcast = self.backcast_linear(theta_b)\n",
    "        forecast = self.forecast_linear(theta_f)\n",
    "        \n",
    "        return backcast, forecast\n",
    "\n",
    "# =============================\n",
    "# Build N-BEATS Model\n",
    "# =============================\n",
    "def create_nbeats_model(lookback, horizon=1, num_stacks=2, num_blocks=2, \n",
    "                        theta_dim=24, hidden_units=48, l2_reg=0.005, dropout_rate=0.4):\n",
    "    inputs = layers.Input(shape=(lookback,))\n",
    "    \n",
    "    residuals = inputs\n",
    "    forecast_outputs = []\n",
    "    \n",
    "    for stack_id in range(num_stacks):\n",
    "        for block_id in range(num_blocks):\n",
    "            backcast, forecast = NBeatsBlock(\n",
    "                units=hidden_units,\n",
    "                theta_dim=theta_dim,\n",
    "                horizon=horizon,\n",
    "                lookback=lookback,\n",
    "                l2_reg=l2_reg,\n",
    "                dropout_rate=dropout_rate,\n",
    "                name=f'nbeats_block_{stack_id}_{block_id}'\n",
    "            )(residuals)\n",
    "            \n",
    "            residuals = layers.Subtract()([residuals, backcast])\n",
    "            forecast_outputs.append(forecast)\n",
    "    \n",
    "    if len(forecast_outputs) > 1:\n",
    "        forecast = layers.Add()(forecast_outputs)\n",
    "    else:\n",
    "        forecast = forecast_outputs[0]\n",
    "    \n",
    "    forecast = layers.Reshape((horizon,))(forecast)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=forecast)\n",
    "    return model\n",
    "\n",
    "model = create_nbeats_model(\n",
    "    lookback=SEQ_LENGTH,\n",
    "    horizon=1,\n",
    "    num_stacks=2,\n",
    "    num_blocks=2,\n",
    "    theta_dim=24,\n",
    "    hidden_units=48,\n",
    "    l2_reg=0.005,\n",
    "    dropout_rate=0.4\n",
    ")\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001, clipnorm=1.0)\n",
    "model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "model.summary()\n",
    "\n",
    "# =============================\n",
    "# Callbacks & Training\n",
    "# =============================\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=20, \n",
    "                          restore_best_weights=True, min_delta=0.00005, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, \n",
    "                             patience=8, min_lr=0.000001, verbose=1)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=40, batch_size=32,\n",
    "                   validation_data=(X_test, y_test),\n",
    "                   callbacks=[early_stop, reduce_lr], verbose=1)\n",
    "\n",
    "# =============================\n",
    "# Predictions & Evaluation\n",
    "# =============================\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "if len(y_pred.shape) > 1 and y_pred.shape[1] > 1:\n",
    "    y_pred = y_pred[:, 0].reshape(-1, 1)\n",
    "else:\n",
    "    y_pred = y_pred.reshape(-1, 1)\n",
    "\n",
    "y_pred_log = scaler.inverse_transform(y_pred)\n",
    "y_test_log = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "y_pred_original = np.exp(y_pred_log) - 1\n",
    "y_test_original = np.exp(y_test_log) - 1\n",
    "\n",
    "mse = mean_squared_error(y_test_original, y_pred_original)\n",
    "mae = mean_absolute_error(y_test_original, y_pred_original)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test_original, y_pred_original)\n",
    "mape = np.mean(np.abs((y_test_original - y_pred_original) / (y_test_original + 1e-8))) * 100\n",
    "\n",
    "print(f\"\\nMSE: {mse:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}, RÂ²: {r2:.4f}, MAPE: {mape:.4f}%\")\n",
    "print(f\"Sample predictions: {y_pred_original[:5].flatten()}\")\n",
    "print(f\"Sample actuals: {y_test_original[:5].flatten()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4732fbc4-8ce2-4274-ac50-f9c058b88b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "plt.xlabel('Epochs', fontsize=12)\n",
    "plt.ylabel('Loss (MSE)', fontsize=12)\n",
    "plt.title('Training vs Validation Loss', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88859e85-ca23-4659-a1fc-30b1eab34f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# STORE N-BEATS RESULTS\n",
    "# =============================\n",
    "# After N-BEATS predictions\n",
    "y_pred_nbeats = y_pred_original.copy()\n",
    "mse_nbeats = mse\n",
    "mae_nbeats = mae\n",
    "rmse_nbeats = rmse\n",
    "r2_nbeats = r2\n",
    "mape_nbeats = mape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb3511d-5156-4943-8565-0b9b9d743228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# LSTM vs N-BEATS Comparison\n",
    "# =============================\n",
    "# Time Series Comparison Plot\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(y_test_original[:500], label='Actual', linewidth=2, alpha=0.7)\n",
    "plt.plot(y_pred_lstm[:500], label='LSTM Predictions', linewidth=1.5, alpha=0.8)\n",
    "plt.plot(y_pred_nbeats[:500], label='N-BEATS Predictions', linewidth=1.5, alpha=0.8)\n",
    "plt.xlabel('Time Steps', fontsize=12)\n",
    "plt.ylabel('PM2.5 Values', fontsize=12)\n",
    "plt.title('LSTM vs N-BEATS: Actual vs Predicted PM2.5', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Metrics Comparison\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL COMPARISON - LSTM vs N-BEATS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Metric':<15} {'LSTM':<20} {'N-BEATS':<20}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'MSE':<15} {mse_lstm:.4f}{'':<15} {mse_nbeats:.4f}\")\n",
    "print(f\"{'RMSE':<15} {rmse_lstm:.4f}{'':<15} {rmse_nbeats:.4f}\")\n",
    "print(f\"{'MAE':<15} {mae_lstm:.4f}{'':<15} {mae_nbeats:.4f}\")\n",
    "print(f\"{'RÂ²':<15} {r2_lstm:.4f}{'':<15} {r2_nbeats:.4f}\")\n",
    "print(f\"{'MAPE (%)':<15} {mape_lstm:.4f}{'':<15} {mape_nbeats:.4f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Determine Winner\n",
    "lstm_score = (rmse_lstm + mae_lstm + mape_lstm) - (r2_lstm * 100)\n",
    "nbeats_score = (rmse_nbeats + mae_nbeats + mape_nbeats) - (r2_nbeats * 100)\n",
    "winner = \"LSTM\" if lstm_score < nbeats_score else \"N-BEATS\"\n",
    "print(f\"\\nðŸ† WINNER: {winner}\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
